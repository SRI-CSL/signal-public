{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SRI-CSL/signal-public/blob/main/colabs/activity-and-roles-detection.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:20em;\"> <strong>SIGNAL</strong><i>ing</i> <code>communication structures</code> in LKML data<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Copyright 2022 SRI International.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is under the GPL3 License. \n",
    "See the [LICENSE](https://www.gnu.org/licenses/gpl-3.0.en.html) file for the full license text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏃‍♀️ Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🪄 Install `needed` libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import typing as ty\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bistiming import Stopwatch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙ Define `processing` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(string: str, length: int = 70) -> str:\n",
    "  \"\"\"Trim a string to the given length.\"\"\"\n",
    "  return (string[:length - 1] + '...') if len(string) > length else string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(datafile: str) -> str:\n",
    "  filename_tuple = os.path.splitext(os.path.basename(datafile))\n",
    "  return filename_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_list(sequence):\n",
    "  \"\"\"Given a sorted list A of elements, return a sorted list B of integers\n",
    "  starting at 0, where each unique value in A has one and only one equivalent\n",
    "  in B\"\"\"\n",
    "\n",
    "  result = list(range(0, len(sequence)))\n",
    "\n",
    "  for i in range(1, len(sequence)):\n",
    "    if sequence[i] == sequence[i-1]:\n",
    "      result[i] = result[i-1]\n",
    "    else:\n",
    "      result[i] = result[i-1] + 1\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_dimension(integer):\n",
    "  \"\"\"Find dimension x of a rectangle that can contain all values of\n",
    "  a range from 1 to integer while keeping rectangle as close to a square as\n",
    "  possible\"\"\"\n",
    "\n",
    "  sqrt = math.floor(math.sqrt(integer))\n",
    "  m1 = 0\n",
    "\n",
    "  for i in reversed(range(1, sqrt+1)):\n",
    "    remainder = integer % i\n",
    "\n",
    "    if remainder == 0:\n",
    "      m1 = i\n",
    "      break\n",
    "\n",
    "  # Prime numbers\n",
    "  if m1 == 1:\n",
    "    m1 = sqrt\n",
    "\n",
    "  return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_list(sequence):\n",
    "  chunk = rect_dimension(len(sequence))\n",
    "  result = list(zip(*[iter(sequence)] * chunk))\n",
    "\n",
    "  diff = len(sequence) % chunk\n",
    "  if diff > 0:\n",
    "    row = sequence[-diff:]\n",
    "\n",
    "    # Add NAN to have a full row of elements\n",
    "    missing = len(result[0]) - len(row)\n",
    "    print(missing)\n",
    "    row.extend([np.nan] * missing)\n",
    "    print(row)\n",
    "\n",
    "    result.append(tuple(row))\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm_transform(data_series: pd.Series) -> ty.Union[ty.Any, tuple]:\n",
    "  \"\"\"Returns the data after applying log and standardization it.\"\"\"\n",
    "  data = np.nan_to_num(data_series)\n",
    "  result = np.nan_to_num(np.log1p(data))\n",
    "  std_scaler = StandardScaler()\n",
    "  std_scaler.fit(result)\n",
    "  return std_scaler.transform(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⊕ Define `plotting` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_count(df, var_list, show=True):\n",
    "  \"\"\"Draw heatmap count of two variables\"\"\"\n",
    "\n",
    "  # Count unique rows\n",
    "  with Stopwatch(\"heatmap_count\"):\n",
    "    count = df.groupby(var_list).size().reset_index(name='n')\n",
    "    count = count.pivot(*var_list, 'n')\n",
    "    sns.heatmap(count)\n",
    "\n",
    "  if show:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_mosaic(df, var_list, show=True):\n",
    "  \"\"\"Ordered mosaic\"\"\"\n",
    "\n",
    "  with Stopwatch(\"Ordered mosaic\"):\n",
    "    # Get values sorted by variables\n",
    "    mosaic = df[var_list].sort_values(var_list).values\n",
    "\n",
    "    # Labels for annotation\n",
    "    labels = ['|'.join([str(elt) for elt in pair]) for pair in mosaic]\n",
    "\n",
    "    # Prepare the matrix with dummy values\n",
    "    values = int_list(labels)\n",
    "\n",
    "    # Give the values and the labels the same shape\n",
    "    values = np.array(rect_list(values))\n",
    "    labels = np.array(rect_list(labels))\n",
    "\n",
    "    mosaic = sns.heatmap(values, annot=labels, fmt='', cbar=False, xticklabels=False, yticklabels=False)\n",
    "    mosaic.set_title('Descending ordered list of {} | {}'.format(*var_list))\n",
    "\n",
    "  if show:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_filter(threads_df: pd.DataFrame, comparator: str = \">\", no_emails_in_threads: int = 10) -> bool:\n",
    "  data_filter_pred = None\n",
    "  if comparator == \">\":\n",
    "    data_filter_pred = threads_df.emails > no_emails_in_threads\n",
    "  elif comparator == \">=\":\n",
    "    data_filter_pred = threads_df.emails >= no_emails_in_threads\n",
    "  elif comparator == \"==\":\n",
    "    data_filter_pred = threads_df.emails == no_emails_in_threads\n",
    "  elif comparator == \"<=\":\n",
    "    data_filter_pred = threads_df.emails <= no_emails_in_threads\n",
    "  elif comparator == \"<\":\n",
    "    data_filter_pred = threads_df.emails < no_emails_in_threads\n",
    "  else:\n",
    "    raise ValueError(f\"unknown comparator {comparator} was given\")\n",
    "  \n",
    "  return data_filter_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_circle(self, components: ty.Tuple[int] =(1, 2)) -> None:\n",
    "  \"\"\"Draw a correlation circle.\n",
    "  :components: PC numbers to be used on the x and y axis\n",
    "  :show: display the graph\n",
    "  \"\"\"\n",
    "\n",
    "  # Create figure\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  # Keep figure square\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  # Center axis around (0,0)\n",
    "  ax.spines['left'].set_position('zero')\n",
    "  ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "  # Eliminate upper and right axes\n",
    "  ax.spines['right'].set_color('none')\n",
    "  ax.spines['top'].set_color('none')\n",
    "\n",
    "  # Keep ticks on bottom and left axis and make them protrude in both directions\n",
    "  ax.xaxis.set_tick_params(bottom=True, top=False, direction='inout')\n",
    "  ax.yaxis.set_tick_params(left=True, right=False, direction='inout')\n",
    "\n",
    "  # Center the graph and make it square\n",
    "  ax.set_yticks([-1.0, -0.8, -0.6, -0.4, -0.2, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "  ax.set_xticks([-1.0, -0.8, -0.6, -0.4, -0.2, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "  # Draw a circle of radius 1\n",
    "  circle1 = plt.Circle((0, 0), radius=1, color='black', fill=False)\n",
    "  ax.add_artist(circle1)\n",
    "\n",
    "  # Get colors for each vector arrow\n",
    "  cmap = plt.cm.get_cmap('Accent', len(self.features))\n",
    "\n",
    "  for feature in range(0, len(self.features)):\n",
    "    # Get coordinates of the \"arrow head\" along both components\n",
    "    x = self.pca.components_[components[0]-1, feature]\n",
    "    y = self.pca.components_[components[1]-1, feature]\n",
    "\n",
    "    # Compute coordinates for annotation\n",
    "    anot_x = x + 0.05 if x > 0 else x - 0.05\n",
    "    anot_y = y + 0.05 if y > 0 else y - 0.05\n",
    "\n",
    "    ax.arrow(0, 0, # Vector arrow starts at (0, 0)\n",
    "             x, y, # Arrow head coordinates\n",
    "             color=cmap(feature),\n",
    "             head_width=0.03, \n",
    "             head_length=0.03)\n",
    "\n",
    "    # Add arrow annotation\n",
    "    ax.annotate(self.features[feature], xy=(anot_x, anot_y), color=cmap(feature))\n",
    "\n",
    "  # Get explained variances as percentage for each component\n",
    "  exp_var_cp1 = round(self.pca.explained_variance_ratio_[components[0]-1] * 100)\n",
    "  exp_var_cp2 = round(self.pca.explained_variance_ratio_[components[1]-1] * 100)\n",
    "\n",
    "  # Label axes with the number of the PC and their % of explained variance\n",
    "  ax.set_xlabel(f'PC {components[0]} ({exp_var_cp1}%)')\n",
    "  ax.set_ylabel(f'PC {components[1]} ({exp_var_cp2}%)')\n",
    "\n",
    "  # Move the axis labels to the edges of the graph\n",
    "  ax.xaxis.set_label_coords(0.5, -0.03)\n",
    "  ax.yaxis.set_label_coords(-0.03, 0.5)\n",
    "\n",
    "  # Give us some room to breathe\n",
    "  ax.set_xlim((-1.02, 1.02))\n",
    "  ax.set_ylim((-1.02, 1.02))\n",
    "\n",
    "  # Give the plot a title\n",
    "  ax.set_title(f'Circle of correlations (PC{components[0]}xPC{components[1]})')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_scatter_plot(self, components: ty.Tuple[int] = (1, 2), color: str = None) -> None:\n",
    "  \"\"\"Draw a scatter plot of points along PC axes.\"\"\"\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  \n",
    "  # Center axis around (0,0)\n",
    "  ax.spines['left'].set_position('zero')\n",
    "  ax.spines['bottom'].set_position('zero')\n",
    "  \n",
    "  # Keep ticks on bottom and left axis and make them protrude in both directions\n",
    "  ax.xaxis.set_tick_params(bottom=True, top=False, direction='inout')\n",
    "  ax.yaxis.set_tick_params(left=True, right=False, direction='inout')\n",
    "  \n",
    "  # Eliminate upper and right axes\n",
    "  ax.spines['right'].set_color('none')\n",
    "  ax.spines['top'].set_color('none')\n",
    "  \n",
    "  # Preparing colors\n",
    "  if color:\n",
    "    color_is_qualitative = isinstance(self.data[color].tolist()[0], str)\n",
    "    colorscheme = 'Spectral'\n",
    "\n",
    "    if color_is_qualitative:\n",
    "      self.data['color'] = pd.factorize(self.data[color])[0]\n",
    "      colorscheme = 'Accent'\n",
    "    else:\n",
    "      self.data['color'] = self.data[color]\n",
    "\n",
    "  if color:\n",
    "    scatter = plt.scatter(\n",
    "      self.projected[:, components[0]-1],\n",
    "      self.projected[:, components[1]-1],\n",
    "      alpha=0.5,\n",
    "      c=self.data.color, edgecolor='none',\n",
    "      cmap=plt.cm.get_cmap(colorscheme, len(self.data[color].unique())))\n",
    "\n",
    "    # Add a colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "\n",
    "    # Label the colorbar and adjust its position\n",
    "    cbar.ax.set_ylabel(color, rotation=90)\n",
    "    cbar.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "    if color_is_qualitative:\n",
    "      # Compute tick position\n",
    "      last_tick = cbar.get_ticks()[-1]\n",
    "      tick_step = last_tick / len(self.data.color.unique())\n",
    "      delta = tick_step / 2\n",
    "      ticks = np.arange(tick_step, last_tick + tick_step, tick_step)\n",
    "\n",
    "      cbar.set_ticks(ticks - delta)\n",
    "      cbar.set_ticklabels(self.data[color].unique())\n",
    "  else:\n",
    "    scatter = plt.scatter(\n",
    "      self.projected[:, components[0]-1],\n",
    "      self.projected[:, components[1]-1],\n",
    "      alpha=0.5)\n",
    "\n",
    "  ax.add_artist(scatter)\n",
    "\n",
    "  # Get explained variances as percentage for each component\n",
    "  exp_var_cp1 = round(self.pca.explained_variance_ratio_[components[0]-1]*100)\n",
    "  exp_var_cp2 = round(self.pca.explained_variance_ratio_[components[1]-1]*100)\n",
    "\n",
    "  # Label the axes with PC number and explained variance %\n",
    "  ax.set_xlabel(f\"PC {components[0]} ({exp_var_cp1}%)\", horizontalalignment='right', x=1)\n",
    "  ax.set_ylabel(f\"PC {components[1]} ({exp_var_cp2}%)\", verticalalignment='bottom', y=0.05)\n",
    "\n",
    "  # Give the plot a title\n",
    "  ax.set_title(f\"PCA (PC{components[0]}xPC{components[1]})\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(self) -> None:\n",
    "  \"\"\"Draw PCA's scree plot\"\"\"\n",
    "\n",
    "  # Create figure\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  # Draw barplot\n",
    "  x = np.arange(len(self.pca.components_))\n",
    "  plt.bar(x, self.pca.explained_variance_ratio_)\n",
    "\n",
    "  # Add explained variance % on top of each bar\n",
    "  for lab_x, lab_y in enumerate(self.pca.explained_variance_ratio_):\n",
    "    ax.text(lab_x, lab_y + 0.03, str(round(lab_y, 2)), horizontalalignment='center')\n",
    "\n",
    "  # Change name of ticks\n",
    "  plt.xticks(x, (f\"PC{str(i+1)}\" for i in x))\n",
    "  ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "\n",
    "  ax.set_title('Scree plot')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👟 Making a `labeled` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛒 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LKML_threads_processed = 'lkmlByEmail_threads_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = f'https://raw.githubusercontent.com/SRI-CSL/signal-public/main/data/{LKML_threads_processed}'\n",
    "if not Path(f'./{LKML_threads_processed}').exists():\n",
    "  # Load preprocessed data and remove unnecessary columns\n",
    "  !wget --no-cache --backups=1 {data_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkml_threads_df = pd.read_csv(LKML_threads_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkml_threads_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔖 `Classifying` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize reconstructed email threads and then classify their communication network structure.  \n",
    "Identified communication network structures will be classified into *6 possible categories*:\n",
    "\n",
    "1. **tree:** tree like discussions\n",
    "2. **atom:** email threads made of a single email\n",
    "3. **comb:** email threads made of emails containing mainly patch data\n",
    "4. **comets:** email threads made of emails containing a combination of patches and some comments\n",
    "5. **stringy:** email threads with back and forth discussions\n",
    "6. **waterfall:** email threads containing large patches\n",
    "\n",
    "These categories are based on the categories listed in [Mailing-List-Analysis Project](https://github.com/gaalcaras/mailingListAnalysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email_threads(threads: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"Categorize threads and then add column 'category' with thread category.\n",
    "  :threads: threads pandas dataframe\n",
    "\n",
    "  Return the categorized threads pandas dataframe\n",
    "  \"\"\"\n",
    "  \n",
    "  # 🌳 The 'default' category: pretty even tree discussions\n",
    "  threads.loc[:, 'category'] = 'tree'\n",
    "\n",
    "  # ⚛ Trivial threads with no depth (they're basically one email threads)\n",
    "  threads.loc[threads.depth == 0, 'category'] = 'atom'\n",
    "\n",
    "  # 🩹 Combs (mainly patches)\n",
    "  threads.loc[threads.deg_max_2 == 0, 'category'] = 'comb'\n",
    "\n",
    "  # ☄ Comets (combs with a tail : mainly patches and some comments)\n",
    "  threads.loc[threads.deg_max_2 == 1, 'category'] = 'comet'\n",
    "\n",
    "  # ◦◦◦◦ Stringy (back and forth threads with no star_nodes)\n",
    "  threads.loc[(threads.deg_max_2 == 1) & (threads.deg_max == 1), 'category'] = 'stringy'\n",
    "\n",
    "  # 🚿 Waterfall (huge patches)\n",
    "  waterfall_cond = ((threads.h_index == 3) & (threads.deg_gini >= 0.7)) | (threads.h_index > 3)\n",
    "  threads.loc[waterfall_cond, 'category'] = 'waterfall'\n",
    "\n",
    "  return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email_threads(lkml_threads_df: pd.DataFrame, debug: bool = False) -> pd.DataFrame:\n",
    "  threads = lkml_threads_df.copy()\n",
    "  threads = classify_email_threads(threads)\n",
    "\n",
    "  cat_count = threads.groupby('category').size().reset_index(name='count')\n",
    "  cat_count = cat_count.sort_values(by='count', ascending=False)\n",
    "  cat_count['%'] = cat_count['count'] * 100 / cat_count['count'].sum()\n",
    "\n",
    "  email_count = threads.groupby('category').emails.sum().reset_index(\n",
    "      name='email_count')\n",
    "  email_count['emails_%'] = email_count[\n",
    "      'email_count'] * 100 / email_count['email_count'].sum()\n",
    "\n",
    "  result = pd.merge(cat_count, email_count, on='category')\n",
    "  if debug:\n",
    "      sys.stdout.write(result.head(10))\n",
    "\n",
    "  return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_lkml_threads_df = process_email_threads(lkml_threads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_lkml_threads_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏗️ Building a Email Threads PCA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍦 `PCA` on email threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadPCA:\n",
    "  def __init__(self, data_df: pd.DataFrame, features: ty.List[str], dimensions: int = 3) -> None:\n",
    "    self.name = \"pca\"\n",
    "\n",
    "    self.data = data_df\n",
    "    self.features = features\n",
    "    self.transf_fn = FunctionTransformer(log_norm_transform)\n",
    "    self.data_t = self.transf_fn.transform(self.data[self.features])\n",
    "    self.table = {}\n",
    "    \n",
    "  def fit_transform(self, dimensions: int = 3) -> ThreadPCA:\n",
    "    with Stopwatch(\"Run PCA\"):\n",
    "      self.pca = PCA(dimensions)\n",
    "      self.pca.fit(self.data_t)\n",
    "      self.projected = self.pca.transform(self.data_t)\n",
    "    \n",
    "    self._build_matching_table()\n",
    "    return self\n",
    "  \n",
    "  def _build_matching_table(self) -> None:\n",
    "    \"\"\"Create a table to allow matching original data points with the\n",
    "    transformed data and their coordinates on each component.\"\"\"\n",
    "\n",
    "    before = self.data[self.features]\n",
    "\n",
    "    # Create dataframe of data after transformation\n",
    "    after = pd.DataFrame(data=self.data_t,\n",
    "                         index=range(0, len(self.data_t)),\n",
    "                         columns=self.features)\n",
    "    after = after.add_suffix('_t')\n",
    "\n",
    "    # Create dataframe of coordinates along each component\n",
    "    coord = pd.DataFrame(data=self.projected,\n",
    "                         index=range(0, len(self.data_t)),\n",
    "                         columns=[f'PC{i+1}' for i in range(len(self.pca.components_))])\n",
    "\n",
    "    # Bind columns from after and before\n",
    "    self.table = pd.concat([after.reset_index(drop=True), before], axis=1)\n",
    "\n",
    "    # Sort columns alphabetically\n",
    "    self.table = self.table[sorted(self.table.columns.tolist())]\n",
    "\n",
    "    # Bind columns from projected data and table\n",
    "    self.table = pd.concat([coord.reset_index(drop=True), self.table], axis=1)\n",
    "\n",
    "    # Count number of occurrences of each row and display it in 'n' column\n",
    "    self.table = self.table.groupby(self.table.columns.tolist()).size().reset_index(name='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍨 Adding `special` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw correlation circle (first two components)\n",
    "ThreadPCA.plot_correlation_circle = plot_correlation_circle\n",
    "# Draw scatter plot (first two components), coloring threads with h_index\n",
    "ThreadPCA.pca_scatter_plot = pca_scatter_plot\n",
    "# Draw scree plot\n",
    "ThreadPCA.scree_plot = scree_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏃‍♀️ Running our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_lkml_threads_copy_df = labeled_lkml_threads_df.copy()\n",
    "data_filter_predicate = build_data_filter(labeled_lkml_threads_copy_df)\n",
    "\n",
    "# Keep only threads containing {operator} {emails_in_threads} emails\n",
    "email_threads_wanted = labeled_lkml_threads_copy_df[data_filter_predicate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a ratio\n",
    "email_threads_wanted['ratio'] = np.log1p(email_threads_wanted['depth']) / np.log1p(email_threads_wanted['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on email thread data\n",
    "pca_threads = ThreadPCA(email_threads_wanted, ['star_nodes', 'ratio', 'deg_max', 'h_index', 'deg_max_2'])\n",
    "pca_threads.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Viewing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_threads.scree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_threads.scatter(color='h_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_threads.scatter(color='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_threads.corr_circle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing some heatmap to explore threads data (e.g., lkml_threads_processed.csv)\n",
    "email_threads_wanted_copy_df = email_threads_wanted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_count(email_threads_wanted_copy_df, ['h_index', 'star_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mosaic(email_threads_wanted_copy_df, ['star_nodes', 'depth'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
