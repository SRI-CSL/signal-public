{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1ac1b9",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Software Introspection for Signaling Emergent Cyber-Social Operations (SIGNAL)</h1>\n",
    "<h2 align=\"center\">SRI International</h2>\n",
    "<h3 align=\"center\">In support of DARPA AIE Hybrid AI to Protect Integrity of Open Source Code (SocialCyber)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b5a56",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">dev2vec: A multi-task learning approach for understanding inter-and-intra developer interactions in open-source software development</h2>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Open-source communities strive to evaluate contributed code on its technical correctness\n",
    "and merits. However, these processes are usually loosely supervised and are often\n",
    "influenced by additional factors, including here but not limited to, social rules, trust,\n",
    "reputation, and arcane processes. These additional components lead to *unexpected*\n",
    "opportunities for subversion. Current approaches, designed to help software development\n",
    "teams to manage the code assessment process, determine the trustworthiness of OSS based on\n",
    "publicly available traces involving code, revision history, logs, and external packages.\n",
    "Although useful, these approaches tend to disregard important inter-and-intra developer\n",
    "interactions that could drastically affect an OSS project's life-cycle. \n",
    "\n",
    "In this notebook, we\n",
    "introduce a systematic approach and a deep multi-task learning architecture, termed\n",
    "**dev2vec**, for identifying and understanding these developer interactions. We\n",
    "empirically evaluate our approach and deep multi-task learning architecture on the Linux\n",
    "Kernel, a mature ecosystem with a vibrant community and a wealth of socio-technical\n",
    "developer interactions, utilizing conversations and interactions originating from its\n",
    "Linux Kernel Mailing List (LKML). Our approach successfully identifies key developer\n",
    "activities determining the distinct roles developers assume in the LKML. \n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "In this section, we motivate the need for understanding the inter-and-intra developer in\n",
    "large OSS projects, particularly in the context of case study: the Linux Kernel and its\n",
    "mailing list.\n",
    "\n",
    "The Linux Kernel(LK) [1] is a mature\n",
    "open-source software with a vibrant community. In the last couple of decades, it has grown\n",
    "and evolved into a large open-source ecosystem. As an ecosystem, it spawns a sufficiently\n",
    "\\textit{diverse set of social and technical interactions} through the involvement of\n",
    "heterogenous community in its Linux Kernel Mailing List (LKML) [2]. It has a hierarchy of mature sub-systems, each of these sub-systems have history\n",
    "of developer interactions and thus can be used *to evidence common behavioral\n",
    "patterns*. For these reasons, we have chosen the LK as our case study. With this case\n",
    "study, we seek to understand how contributions are carried out over time and thus capture\n",
    "common behavioral patterns.\n",
    "\n",
    "Motivated by these considerations, our work builds upon a general work in Activity-based\n",
    "analysis of OSS projects [3], specializes it to LK case study, and\n",
    "then extends it with a deep multi-task learning architecture to model developer behavior and\n",
    "thus capture role dynamism over a user-specified time window. The latter is inspired by\n",
    "work in user simulation using recurrent neural networks [4]. In this user\n",
    "simulation work, recurrent neural networks are applied to describing users in a given\n",
    "domain without expert supervision [5].\n",
    "\n",
    "To establish this understanding of developer actions, roles, and these roles dynamism, we\n",
    "first collect and curate data from the LKML within a user-specified time window. These\n",
    "data contains key attributes concerning communication and interactions among LK\n",
    "developers. We then transform them to make sure we can learn a function that maps a sequence\n",
    "of past developer activity as input to a series of observations, within the specified\n",
    "window. This transformation effort aims capturing concrete actions taken by LK developers from\n",
    "different perspectives, which have grown and evolved over time. In the Table below, we enlist the transformed data.\n",
    "\n",
    "| Metric             | Description                                                                  |\n",
    "|--------------------|------------------------------------------------------------------------------|\n",
    "| message_exper      | # of patch emails sent by a developer in earlier email threads.              |\n",
    "| commit_exper.      | # of committed patches by a developer.                                       |\n",
    "| fkre_score.        | Avg. Flesch Kincaid Reading Ease score (text comprehension difficulty).      |\n",
    "| fkgl_score.        | Avg. Flesch Kincaid Grade Level Score (text reading grade level).            |\n",
    "| verbosity.         | Wordiness level of a patch email (ratio of # words against # sentences).     |\n",
    "| exert_persuasion   | Measures whether influence has been exerted via persuasion strategies.       |\n",
    "| sent_time          | Year/month/week of the year/Day of week in which a patch email was sent.     |\n",
    "| received_time      | Year/month/week of the year/Day of week in which a patch email was received. |\n",
    "| reply_within_4hrs  | Measures whether sent out patch email receives a reply email within 4 hrs.   |\n",
    "| patch_email        | Measures whether an email a patch email.                                     |\n",
    "| first_patch_thread | Measures whether a patch email initiated an email thread.                    |\n",
    "| patch_churn        | Measures whether a patch in email is a patch rewrite (update).               |\n",
    "| bug_fix            | Measures whether the patch in email is a bug fix patch.                      |\n",
    "| new_feature        | Measures whether the patch in email is a new feature patch.                  |\n",
    "| accepted_patch     | Measures whether patch an email is an accepted patch in LKML.                |\n",
    "| accepted_commit    | Measures whether patch in email is a committed patch in LK.                  |\n",
    "\n",
    "A preliminary analysis on the transformed data indicated that the collected metrics are\n",
    "inter-related and thus can be determined by a set of *latent factors*. Based on\n",
    "this finding, we hypothesize that *these latent factors are the typical activities\n",
    "that LK developers engage in when assuming certain roles in the LKML*. To help us\n",
    "understand these latent inter-relations, we perform factor\n",
    "analysis [6]. Factor analysis is an exploratory data analysis\n",
    "method that we can use for both *extracting* latent factors from our\n",
    "data and simplifying (i.e., *rotating*) the structure of these data to\n",
    "improve their interpretability. Then, we use its output to identify the typical activities in\n",
    "the LKML.\n",
    "\n",
    "Using these typical activities, we design a deep multi-task learning architecture for\n",
    "modeling the dynamic nature of activities in the LKML. Multi-task learning (MTL) is a\n",
    "subfield of machine learning where a model attempts to concurrently learn a series of\n",
    "*related* tasks [7, 8, 9], e.g., simultaneously learning the evaluate the distance, spin, and\n",
    "trajectory of a ball in a ping-pong game [10, 11].\n",
    "Note that each tasks has it's own loss function. In the context of MTL, we seek to\n",
    "minimize the sum of the loss functions [9]. MTL is well-suited\n",
    "for the problem at hand, given that we are interested in asking a series of questions\n",
    "related to future developer activity.\n",
    "\n",
    "## dev2vec\n",
    "\n",
    "We now move on to  introducing ***dev2vec***, our LSTM-based multi-task learning model designed to learn from a sequence of past developer activities and output a series of observations corresponding to the upcoming (future) developer actions. Note that, we use the term *dev2vec* interchangeably referring to the data representation or the deep MTL architecture, depending on the context.\n",
    "\n",
    "![dev2vec](figures/dev2vec.png)\n",
    "\n",
    "The design of *dev2vec*'s architecture, Figure above, is inspired by the work of Zolna et\n",
    "al. [5]. In this work, the authors use an Long-Short-Term Memory\n",
    "(LSTM)-based MTL architecture to describe to describe the characteristics of users\n",
    "involved in Real-Time Bidding online auction events. Similarly, we aim at having a model\n",
    "capable of answering questions of interest regarding future developer actions.\n",
    "\n",
    "Our deep MTL model is sequential in structure. It takes as input a sequence of vectors\n",
    "composed of $n$-floating point numbers in the $[-1, 1]$ range. Here, the term $n$ refers\n",
    "to the number of features depicting developer characteristics that we extracted through\n",
    "factor analysis. The input is fed to an LSTM layer with 300 memory cells, applying a\n",
    "dropout of $0.15$. Differently from *user2vec*, our second LSTM layer also contains\n",
    "300-memory cells. The output of the last LSTM layer is then fed as input to 3-different\n",
    "fully connected (FC) layers, or *heads*, as commonly referred in the literature. Each\n",
    "FC model is trained to answer to one of the following questions of interest:\n",
    "\n",
    "1. Will the next action be a triage action?\n",
    "2. Will the next action result in a small code change?\n",
    "3. Will the next action result in a controversial code change, sparking a lot of discussions?\n",
    "\n",
    "We bring to the attention of the reader that the architecture can be easily extended to accommodate different tasks of interest.\n",
    "\n",
    "#### References\n",
    "\n",
    "[1] The Linux Kernel. https://github.com/torvalds/linux\n",
    "\n",
    "[2] The Linux Kernel Mailing List. https://lkml.org/\n",
    "\n",
    "[3] Cheng, Jinghui, and Jin LC Guo. \"*Activity-based analysis of open source software contributors: Roles and dynamics.*\" In 2019 IEEE/ACM 12th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE), pp. 11-18. IEEE, 2019. [PDF](https://arxiv.org/pdf/1903.05277.pdf)\n",
    "\n",
    "[4] Gür, Izzeddin, Dilek Hakkani-Tür, Gokhan Tür, and Pararth Shah. \"*User modeling for task oriented dialogues.*\" In 2018 IEEE Spoken Language Technology Workshop (SLT), pp. 900-906. IEEE, 2018. [PDF](https://arxiv.org/pdf/1811.04369.pdf)\n",
    "\n",
    "[5] Żołna, Konrad, and Bartłomiej Romański. \"*User modeling using LSTM networks.*\" In Thirty-First AAAI Conference on Artificial Intelligence. 2017. [PDF](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14220/14254)\n",
    "\n",
    "[6] Child, Dennis. *The essentials of factor analysis*. Cassell Educational, 1990.\n",
    "\n",
    "[7] Crawshaw, Michael. \"Multi-task learning with deep neural networks: A survey.\" arXiv preprint arXiv:2009.09796 (2020). [PDF](https://arxiv.org/pdf/2009.09796.pdf)\n",
    "\n",
    "[8] Standley, Trevor, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. \"*Which tasks should be learned together in multi-task learning?.*\" In International Conference on Machine Learning, pp. 9120-9132. PMLR, 2020. [PDF](http://proceedings.mlr.press/v119/standley20a/standley20a.pdf)\n",
    "\n",
    "[9] Soni, Devin. Multi-task learning in Machine Learning. https://towardsdatascience.com/multi-task-learning-in-machine-learning-20a37c796c9c\n",
    "\n",
    "[10] Fifty, Christopher. Deciding Which Tasks Should Train Together in Multi-Task Neural Networks. https://ai.googleblog.com/2021/10/deciding-which-tasks-should-train.html\n",
    "\n",
    "[11] Fifty, Chris, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. \"*Efficiently identifying task groupings for multi-task learning.*\" Advances in Neural Information Processing Systems 34 (2021). [PDF](https://proceedings.neurips.cc/paper/2021/file/e77910ebb93b511588557806310f78f1-Paper.pdf)\n",
    "\n",
    "\n",
    "#### Disclaimer\n",
    "\n",
    "The content of this notebook is released under the **GNU General Public License v3.0**, see [LICENSE](https://github.com/SRI-CSL/signal-public/blob/main/LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212e519",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6697ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional.classification import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92543867",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(PARENT_DIR)\n",
    "\n",
    "DATASETS_DIR = os.path.join(PARENT_DIR, 'data')\n",
    "MODELS_DIR = os.path.join(PARENT_DIR, 'models')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceefb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 4\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "nb_epochs = 10\n",
    "weight_decay = 1e-4\n",
    "\n",
    "loss_function = [nn.CrossEntropyLoss(), nn.MSELoss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f519ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_df, labels_df, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.X = torch.tensor(data_df[self.features].values).float()\n",
    "        self.y = torch.tensor(labels_df[self.target].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        # output: input_data, target_0, target_1, target_2\n",
    "        return x, self.y[i][0], self.y[i][1], self.y[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f303e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dev2Vec(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n",
    "        super(Dev2Vec, self).__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=0.15\n",
    "        )\n",
    "        \n",
    "        # Heads\n",
    "        # fc0: is_triage\n",
    "        self.fc0 = nn.Linear(in_features=n_hidden, out_features=30)\n",
    "        self.fc0_0 = nn.Linear(in_features=30, out_features=2)\n",
    "        nn.init.xavier_normal_(self.fc0.weight)\n",
    "        nn.init.xavier_normal_(self.fc0_0.weight)\n",
    "        \n",
    "        # fc1: is_bug_fix\n",
    "        self.fc1 = nn.Linear(in_features=n_hidden, out_features=30)\n",
    "        self.fc1_0 = nn.Linear(in_features=30, out_features=2)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc1_0.weight)\n",
    "        \n",
    "        # fc2: is_controversial\n",
    "        self.fc2 = nn.Linear(in_features=n_hidden, out_features=30)\n",
    "        self.fc2_0 = nn.Linear(in_features=30, out_features=2)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc2_0.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "        h0 = torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device)\n",
    "        c0 = torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(sequences.view(len(sequences), self.seq_len, -1), (h0, c0))\n",
    "        last_time_step = lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
    "        \n",
    "        #y0 = F.relu(self.fc0(last_time_step))\n",
    "        y0 = self.fc0(last_time_step)\n",
    "        y0 = F.relu(self.fc0_0(y0))\n",
    "        y0 = F.softmax(y0, dim=1)\n",
    "        \n",
    "        y1 = F.relu(self.fc1(last_time_step))\n",
    "        #y1 = self.fc1(last_time_step)\n",
    "        y1 = F.relu(self.fc1_0(y1))\n",
    "        y1 = F.softmax(y1, dim=1)\n",
    "        \n",
    "        #y2 = F.relu(self.fc2(last_time_step))\n",
    "        y2 = self.fc2(last_time_step)\n",
    "        y2 = F.relu(self.fc2_0(y2))\n",
    "#         y2 = self.fc2_0(y2)\n",
    "        y2 = F.softmax(y2, dim=1)        \n",
    "        \n",
    "        return y0, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faab50e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6404 entries, 0 to 6403\n",
      "Data columns (total 17 columns):\n",
      " #   Column                        Non-Null Count  Dtype              \n",
      "---  ------                        --------------  -----              \n",
      " 0   sent_time                     6404 non-null   datetime64[ns, UTC]\n",
      " 1   Unnamed: 0                    6404 non-null   float64            \n",
      " 2   Unnamed: 0.1                  6404 non-null   float64            \n",
      " 3   sender_id                     6404 non-null   float64            \n",
      " 4   Code Contribution             6404 non-null   float64            \n",
      " 5   Knowledge Sharing             6404 non-null   float64            \n",
      " 6   Patch Posting                 6404 non-null   float64            \n",
      " 7   Progress Control              6404 non-null   float64            \n",
      " 8   Acknowledgement and Response  6404 non-null   float64            \n",
      " 9   Composite Index               6404 non-null   float64            \n",
      " 10  Rank                          6404 non-null   float64            \n",
      " 11  Status                        6404 non-null   object             \n",
      " 12  comb_2                        6404 non-null   float64            \n",
      " 13  comb_3                        6404 non-null   float64            \n",
      " 14  is_triage                     6404 non-null   int64              \n",
      " 15  is_bug_fix                    6404 non-null   int64              \n",
      " 16  is_controversial              6404 non-null   int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(12), int64(3), object(1)\n",
      "memory usage: 850.7+ KB\n",
      "- Out of 17-features present in the original DataFrame, we consider 5-features.\n",
      "- We have 3-targets.\n",
      "(6304, 5) (6304, 3) (100, 5) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "model_save_path = os.path.join(MODELS_DIR, 'dev2vec-model-activity-V2.pth')\n",
    "\n",
    "# path_activities_data = os.path.join(DATASETS_DIR, 'path_activities_5min_window.csv')\n",
    "path_activities_data = os.path.join(DATASETS_DIR, 'activities_5min_window.csv')\n",
    "activities_df = pd.read_csv(path_activities_data, sep='\\t')\n",
    "\n",
    "activities_df['sent_time'] = pd.to_datetime(activities_df['sent_time'], utc=True)\n",
    "\n",
    "activities_df.info()\n",
    "\n",
    "features_of_interest = ['Code Contribution', 'Knowledge Sharing', 'Patch Posting',\n",
    "                        'Progress Control', 'Acknowledgement and Response']\n",
    "\n",
    "targets_of_interest = ['is_triage', 'is_bug_fix', 'is_controversial']\n",
    "\n",
    "print(f\"- Out of {activities_df.shape[1]}-features present in the original DataFrame, \"\n",
    "      f\"we consider {len(features_of_interest)}-features.\\n\"\n",
    "      f\"- We have {len(targets_of_interest)}-targets.\")\n",
    "\n",
    "features_df = activities_df[features_of_interest].copy()\n",
    "targets_df = activities_df[targets_of_interest].copy()\n",
    "\n",
    "test_data_size = 100\n",
    "\n",
    "train_data = features_df[:-test_data_size]\n",
    "train_labels = targets_df[:-test_data_size]\n",
    "\n",
    "test_data = features_df[-test_data_size:]\n",
    "test_labels = targets_df[-test_data_size:]\n",
    "\n",
    "print(train_data.shape, train_labels.shape, test_data.shape, test_labels.shape)\n",
    "\n",
    "# creating the training dataset sequence\n",
    "train_dataset = SequenceDataset(\n",
    "    data_df=train_data,\n",
    "    labels_df=train_labels,\n",
    "    target=targets_of_interest,\n",
    "    features=features_of_interest,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "test_dataset = SequenceDataset(\n",
    "    data_df=test_data,\n",
    "    labels_df=test_labels,\n",
    "    target=targets_of_interest,\n",
    "    features=features_of_interest,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# creating the train and test DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6114b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dev2Vec(\n",
       "  (lstm): LSTM(5, 300, num_layers=2, dropout=0.15)\n",
       "  (fc0): Linear(in_features=300, out_features=30, bias=True)\n",
       "  (fc0_0): Linear(in_features=30, out_features=2, bias=True)\n",
       "  (fc1): Linear(in_features=300, out_features=30, bias=True)\n",
       "  (fc1_0): Linear(in_features=30, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=30, bias=True)\n",
       "  (fc2_0): Linear(in_features=30, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = Dev2Vec(n_features=len(features_of_interest), n_hidden=300, seq_len=sequence_length)\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c4a3375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing set: 100%|█████████████████████████████████████████████████| 25/25 [00:00<00:00, 359.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- testing accuracy triage: 0.660\n",
      "- testing accuracy bug: 0.690\n",
      "- testing accuracy controversial: 1.000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = list()\n",
    "acc_triage = list()\n",
    "acc_bug = list()\n",
    "acc_controversial = list()\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_loader, ncols=100, desc=f\"Testing set\"):\n",
    "    x, y0, y1, y2 = batch\n",
    "#     print(x)\n",
    "\n",
    "    # 1. Forward\n",
    "    # y_hat or logits (l)\n",
    "    with torch.no_grad():\n",
    "        l_0, l_1, l_2 = model(x) \n",
    "\n",
    "        # 2. Compute the objective function, i.e., J\n",
    "        J_0 = loss_function[0](l_0, y0)\n",
    "        J_1 = loss_function[0](l_1, y1)\n",
    "        J_2 = loss_function[0](l_2, y2)\n",
    "\n",
    "        # 4. accumulate the partial derivatives of J wrt params\n",
    "        J = J_0 + J_1 + J_2\n",
    "\n",
    "        losses.append(J.item())\n",
    "\n",
    "        acc_0 = y0.eq(l_0.detach().argmax(dim=1)).float().mean()\n",
    "        acc_1 = y1.eq(l_1.detach().argmax(dim=1)).float().mean()\n",
    "        acc_2 = y2.eq(l_2.detach().argmax(dim=1)).float().mean()\n",
    "        #accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "        acc_triage.append(acc_0)\n",
    "        acc_bug.append(acc_1)\n",
    "        acc_controversial.append(acc_2)\n",
    "\n",
    "# print(f\"testing loss: {torch.tensor(losses).mean():.3f}\")\n",
    "print(f\"- testing accuracy triage: {torch.tensor(acc_triage).mean():.3f}\")\n",
    "print(f\"- testing accuracy bug: {torch.tensor(acc_bug).mean():.3f}\")\n",
    "print(f\"- testing accuracy controversial: {torch.tensor(acc_controversial).mean():.3f}\")\n",
    "print(f\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49136179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
